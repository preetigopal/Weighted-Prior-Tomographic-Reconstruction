\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\bibliographystyle{IEEEtran}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{boxedminipage}
\usepackage{a4wide}
\usepackage{subcaption}
\usepackage[colorlinks=true]{hyperref}
\title{Response to Reviewers and Associate Editor}
\begin{document}
\date{}
\maketitle

This paper was submitted earlier, and out of the three reviewers, two of them had pronounced this as ``publish unaltered'' and one reviewer (R1) did not recommend resubmission.

The handling Editor has encouraged a resubmission of this paper taking into account the comments of R1.  In particular, R1 has these concerns and we quote these
\begin{quote}
  The authors have spent considerable effort in revising the
  manuscript and addressing the concerns of the reviewers. I would
  like to thank the authors for their effort. I believe the manuscript
  has improved because of these revisions, especially in the structure
  of the paper and the discussion of related work. Unfortunately, some
  of my main concerns with regards to the experiments, the accuracy of
  the proposed method, and the presentation of the findings remain
  present in the revised manuscript. In some cases the revisions have
  even increased my concerns about certain aspects. Mainly, the
  included experiments still do not clearly show that the proposed
  method improves upon existing ‘simple’ prior-based methods, and the
  new results even show that there is evidence that FDK is more
  accurate than the proposed method exactly in the region that changed
  \end{quote}

In this document, we explain the steps we have taken to address the
reviews received in our previous submission to TCI. In particular,
we address the main concern of the reviewer below.

\begin{itemize}
\item \textbf{Simple methods are better, why yours?}
  
The Reviewer has taken pains in showing how the Total-Variation method
is better than our earlier Compressed Sensing plus spatially-varying
prior-based reconstruction. In some cases FDK is more accurate.

This is correct for the dataset shown, and we accept this to be true.

  That said, the goal of our previous and curent work is to provide a
  technique that can improve upon a chosen baseline reconstruction
  when measurements are extremely sparse. One may choose any baseline
  reconstruction for pedagogical, historical, or commercial reasons,
  and is there a way to improve?
  
  Nevertheless, after noting reviewer's comments, we conducted further
  analysis and observed that TV is indeed better suited as a baseline
  reconstruction method for our datasets and our subsampling
  choices. Hence, our current paper presents all results using TV
  regularization coupled with spatially-varying technique, and
  demonstrates its benefits over TV-only and backprojection-only
  methods.  We have eliminated our previous implicit claim that the CS
  based scheme is optimal for the dataset provided.

  As far as FDK is concerned, the current results indeed show the
  superiority of the TV-based scheme (as also noted by the reviewer).
  
\item \textbf{Other comments given by Reviewer R1 and the handling editor}

 \begin{itemize}
  \item \textbf{Machine Learning}
  
  In a machine learning based setting, our goal of detecting new
  changes is a `prediction' problem in a continuous solution space
  i.e., \textit{``given intensities of a voxel at various time
    instants in a longitudinal setting and partial measurements of the
    voxel at the current time, what will be the intensity of the same
    voxel at the current time instant?''} This estimation can be
  learnt by a deep neural network if there are hundreds of labeled
  data.  However, generalization of this across multiple datasets
  poses a question mark. 
  
  %% thousands of time instants at which the previous voxel intensities
  %% are known for each voxel. This amount of data is not available to
  %% us, and may be hard to acquire because each specimen is usually
  %% scanned only a couple of times in any longitudinal study.

  Further, a specific `event-of-interest' may occur just once in a
  longitudinal study and hence training on data of all previous time
  instants may be misleading. If however, we use measurements from
  identical and full-cycle longitudinal studies (including
  events-of-interest) of similar specimen (instead of the `same'
  specimen as we used), a deep-learning technique may be applied. We
  see this as an extension of using object-prior generated from the
  same object, and hence we have not explored this direction.

  Another avenue for using deep-networks \textit{within} our current
  work is to replace the currently used fixed eigenspace provided by
  PCA by a learnt feature basis provided by a network such as an
  autoencoder. For each dataset, an autoencoder may be trained to
  learn a specific set of latent features (this might again require
  atleast a few tens of volumes). We see this as a future direction of
  work.

\item \textbf{Details of other methods used for comparison}
 We have compared our method with Algebraic Reconstruction Technique
 (ART), Simultaneous Algebraic Reconstruction Technique (SART),
 Simultaneous Iterative Reconstruction Technique (SIRT), Total
 Variation (TV) and Compressed Sensing (CS) with Haar wavelet and DCT
 as sparsity basis. The details of solvers used for these
 algorithms are now mentioned in Page.~7 (Section-6A). For CS
 reconstruction, we observed that Haar wavelet and DCT were best
 suited for our dataset among various other possible sparsity basis.

\item \textbf{Discussion about Metrics used}
  In our experiments, we observed that for a given dataset and a set of projection measurements, the intensity span of histograms of reconstructed volumes differ across various methods and solvers. Hence, we choose to focus on preserving the structures on our reconstructions, and not rely on the absolute intensity values alone. Hence, we choose SSIM with a higher weightage to structure-preservation. For our record, we have computed RMSE values as well and they are presented here in Tables~\ref{table:potato_rmse} and~\ref{table:sprouts_rmse}.

\begin{table}[!h]
  \centering
 \caption{RMSE within the RoI of 3D reconstructed Potato volume from various
    methods.}
\begin{tabular}{|l|c|c|c|c|}
\hline 
\textbf{Backprojection} & \textbf{TV} & \textbf{This paper}
\\ \hline  0.29 & 0.24 & \textcolor{red}{0.18}
\\ \hline
\end{tabular}
\label{table:potato_rmse}
\end{table}

\begin{table}[!h]
  \centering
 \caption{RMSE within the RoI of 3D reconstructed Sprouts volume from various
    methods.}
\begin{tabular}{|l|c|c|c|c|}
\hline 
\textbf{Backprojection} & \textbf{TV} & \textbf{This paper}
\\ \hline  0.53 & 0.48 & \textcolor{red}{0.30}
\\ \hline
\end{tabular}
\label{table:sprouts_rmse}
\end{table}


\item \textbf{Dicsussion about Computational Cost}\\
  We now present details of both time and space complexities of our technique in Section.7B of our paper.
\end{itemize}
  
  
\end{itemize}

\end{document}

