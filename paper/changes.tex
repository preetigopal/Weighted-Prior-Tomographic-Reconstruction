\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\bibliographystyle{IEEEtran}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{boxedminipage}
%\usepackage{geometry}
\usepackage[colorlinks=true]{hyperref}
\title{Changes after the last submission}
\begin{document}
\maketitle
In this document, we explain the steps we have taken to address the reviews received in our previous submission to TCI. In particular, there were two major concerns:

\begin{itemize}
\item \textbf{Total-Variation being better than CS + spatially-varying prior-based reconstruction:}\\

  The goal of our previous and curent work is to provide a technique that can improve upon any chosen baseline reconstruction when measurements are few. After noting reviewer's comments, we conducted further analysis and observed that TV is indeed better suited as a baseline reconstruction method for our datasets and our subsampling choices. Hence, our current paper presents all results using TV regularization coupled with spatially-varying technique, and demonstrates its benefits over TV-only and backprojection-only methods.

\item \textbf{Comparison with Deep Learning Techniques:}\\
  
  In a machine learning based setting, our goal of detecting new changes is a `prediction' problem in a continuous solution space i.e., \textit{``given intensities of a voxel at various time instants in a longitudinal setting and partial measurements of the voxel at the current time, what will be the intensity of the same voxel at the current time instant?''} This estimation can be learnt by a deep neural network only if there are hundreds if not, thousands of time instants at which the previous voxel intensities are known for each voxel. This amount of data is not available to us, and may be hard to acquire because each specimen is usually scanned only a couple of times in any longitudinal study.

  Further, a specific `event-of-interest' may occur just once in a longitudinal study and hence training on data of all previous time instants may be misleading. If however, we use measurements from identical and full-cycle longitudinal studies (including events-of-interest) of similar specimen (instead of the `same' specimen as we used), a deep-learning technique may be applied. We see this as an extension of using object-prior generated from the same object, and hence we have not explored this direction.

  Another avenue for using deep-networks \textit{within} our current work is to replace the currently used fixed eigenspace provided by PCA by a learnt feature basis provided by a network such as an autoencoder. For each dataset, an autoencoder may be trained to learn a specific set of latent features (this might again require atleast a few tens of volumes). We see this as a future direction of work. 

  
  
\end{itemize}

\end{document}

